{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install gdown","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-06T10:53:34.858958Z","iopub.execute_input":"2022-06-06T10:53:34.859351Z","iopub.status.idle":"2022-06-06T10:54:06.670941Z","shell.execute_reply.started":"2022-06-06T10:53:34.85928Z","shell.execute_reply":"2022-06-06T10:54:06.669974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/pytorch/vision.git\n%cd vision\n!git checkout v0.3.0\n\n!cp references/detection/utils.py ../\n!cp references/detection/transforms.py ../\n!cp references/detection/coco_eval.py ../\n!cp references/detection/engine.py ../\n!cp references/detection/coco_utils.py ../\n\n%cd ..","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install timm","metadata":{"execution":{"iopub.status.busy":"2022-06-05T04:59:56.681365Z","iopub.execute_input":"2022-06-05T04:59:56.682021Z","iopub.status.idle":"2022-06-05T05:00:07.413923Z","shell.execute_reply.started":"2022-06-05T04:59:56.681986Z","shell.execute_reply":"2022-06-05T05:00:07.412955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip3 install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cu102","metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gdown\n\nurl = 'https://drive.google.com/uc?id=1maXjFAFcPhXG8B4F1emu_cLq6X06OW_p'\n\noutput = 'dataset.zip'\ngdown.download(url, output)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T10:56:19.973265Z","iopub.execute_input":"2022-06-06T10:56:19.973667Z","iopub.status.idle":"2022-06-06T10:57:08.108543Z","shell.execute_reply.started":"2022-06-06T10:56:19.973633Z","shell.execute_reply":"2022-06-06T10:57:08.106511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip dataset.zip >/dev/null","metadata":{"execution":{"iopub.status.busy":"2022-06-06T10:57:08.110361Z","iopub.execute_input":"2022-06-06T10:57:08.110932Z","iopub.status.idle":"2022-06-06T10:57:26.760636Z","shell.execute_reply.started":"2022-06-06T10:57:08.110893Z","shell.execute_reply":"2022-06-06T10:57:26.759542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**References** <br>\nhttps://pytorch.org/tutorials/intermediate/torchvision_tutorial.html <br>\nhttps://haochen23.github.io/2020/06/fine-tune-mask-rcnn-pytorch.html <br>\nhttps://discuss.pytorch.org/t/multi-class-implementation-mask-rcnn/70476","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport torch.utils.data\nfrom PIL import Image\nimport transforms as T\n\n\nclass DocumentDataset(torch.utils.data.Dataset):\n    def __init__(self, root, transforms=None):\n        self.root = root\n        self.transforms = transforms\n        # load all image files, sorting them to\n        # ensure that they are aligned\n        self.imgs = list(sorted(os.listdir(os.path.join(root, \"images\"))))\n        self.masks = list(sorted(os.listdir(os.path.join(root, \"masks\"))))\n\n    def __getitem__(self, idx):\n        # load images ad masks\n        img_path = os.path.join(self.root, \"images\", self.imgs[idx])\n        mask_path = os.path.join(self.root, \"masks\", self.masks[idx])\n        img = Image.open(img_path).convert(\"RGB\")\n        # note that we haven't converted the mask to RGB,\n        # because each color corresponds to a different instance\n        # with 0 being background\n        mask = Image.open(mask_path)\n        if self.transforms is not None:\n            img_np = np.array(img)\n            mask_np = np.array(mask)\n            transformed = self.transforms(image=img_np, mask=mask_np)\n            img = Image.fromarray(transformed[\"image\"])\n            mask = Image.fromarray(transformed[\"mask\"])\n        # convert the PIL Image into a numpy array\n        mask = np.array(mask)\n        \n        # instances are encoded as different colors\n        obj_ids = np.unique(mask)\n        # first id is the background, so remove it\n        obj_ids = obj_ids[1:]\n\n        # split the color-encoded mask into a set\n        # of binary masks\n        masks = mask == obj_ids[:, None, None]\n\n        # get bounding box coordinates for each mask\n        num_objs = len(obj_ids)\n        boxes = []\n        for i in range(num_objs):\n            pos = np.where(masks[i])\n            xmin = np.min(pos[1])\n            xmax = np.max(pos[1])\n            ymin = np.min(pos[0])\n            ymax = np.max(pos[0])\n            boxes.append([xmin, ymin, xmax, ymax])\n\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        # there is only one class\n        # labels = torch.ones((num_objs,), dtype=torch.int64)\n        # there are now 11 classes (background + 10 documents)\n        labels = torch.as_tensor(obj_ids, dtype=torch.int64)\n        masks = torch.as_tensor(masks, dtype=torch.uint8)\n\n        image_id = torch.tensor([idx])\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"masks\"] = masks\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n        \n        trans = T.Compose([T.ToTensor()])\n        img, target = trans(img, target)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.imgs)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T12:58:34.578853Z","iopub.execute_input":"2022-06-06T12:58:34.579223Z","iopub.status.idle":"2022-06-06T12:58:34.599785Z","shell.execute_reply.started":"2022-06-06T12:58:34.579192Z","shell.execute_reply":"2022-06-06T12:58:34.598953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\nfrom torchvision.models.detection import MaskRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\n# import timm\n# import torch\n\ndef build_model(num_classes):\n    # load an instance segmentation model pre-trained on COCO\n    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n    # Available in nightly version\n    # model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(pretrained=True)\n\n    # get the number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    # Stop here if you are fine-tunning Faster-RCNN\n\n    # now get the number of input features for the mask classifier\n    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n    hidden_layer = 256\n    # and replace the mask predictor with a new one\n    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n                                                       hidden_layer,\n                                                       num_classes)\n\n    return model\n\n# def build_model(num_classes):\n#     # load a pre-trained model for classification and return\n#     # only the features\n#     # m = timm.create_model('resnet50', pretrained=True)\n#     # backbone = torch.nn.Sequential(*(list(m.children())[:-2]))\n#     # backbone = torchvision.models.efficientnet_b4(pretrained=True).features\n#     # backbone = timm.create_model('resnet50', features_only=True, pretrained=True)\n#     # backbone.out_channels = 2048\n    \n#     backbone = timm.create_model('resnet50', pretrained=True)\n#     backbone.reset_classifier(0, '')\n#     backbone.out_channels = backbone.num_features\n\n#     # let's make the RPN generate 5 x 3 anchors per spatial\n#     # location, with 5 different sizes and 3 different aspect\n#     # ratios. We have a Tuple[Tuple[int]] because each feature\n#     # map could potentially have different sizes and\n#     # aspect ratios\n#     anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n#                                        aspect_ratios=((0.5, 1.0, 2.0),))\n\n#     # let's define what are the feature maps that we will\n#     # use to perform the region of interest cropping, as well as\n#     # the size of the crop after rescaling.\n#     # if your backbone returns a Tensor, featmap_names is expected to\n#     # be [0]. More generally, the backbone should return an\n#     # OrderedDict[Tensor], and in featmap_names you can choose which\n#     # feature maps to use.\n#     roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n#                                                     output_size=7,\n#                                                     sampling_ratio=2)\n    \n#     mask_roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n#                                                          output_size=14,\n#                                                          sampling_ratio=2)\n\n#     # put the pieces together inside a MaskRCNN model\n#     model = MaskRCNN(backbone,\n#                      num_classes=num_classes,\n#                      rpn_anchor_generator=anchor_generator,\n#                      box_roi_pool=roi_pooler,\n#                      mask_roi_pool=mask_roi_pooler)\n    \n#     return model","metadata":{"execution":{"iopub.status.busy":"2022-06-06T12:58:38.543175Z","iopub.execute_input":"2022-06-06T12:58:38.543586Z","iopub.status.idle":"2022-06-06T12:58:38.557015Z","shell.execute_reply.started":"2022-06-06T12:58:38.543552Z","shell.execute_reply":"2022-06-06T12:58:38.55572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from engine import train_one_epoch, evaluate\nimport os\nimport utils\n# import torchvision.transforms as T\nimport albumentations as A\n\n\ndef get_transform(train):\n    transforms = []\n    # data augmentation\n    if train:\n        transforms.append(A.Rotate(interpolation=0, border_mode=0, value=0, mask_value=0))\n    return A.Compose(transforms)\n\n# use our dataset and defined transformations\ndataset = DocumentDataset(os.path.join('dataset', 'train'), get_transform(train=True))\ndataset_test = DocumentDataset(os.path.join('dataset', 'test'), get_transform(train=False))\n\n# split the dataset in train and test set\n# torch.manual_seed(1)\n# indices = torch.randperm(len(dataset)).tolist()\n# dataset = torch.utils.data.Subset(dataset, indices[:-50])\n# dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n\n# define training and validation data loaders\ndata_loader = torch.utils.data.DataLoader(\n    dataset, batch_size=2, shuffle=True, num_workers=os.cpu_count(),\n    collate_fn=utils.collate_fn)\n\ndata_loader_test = torch.utils.data.DataLoader(\n    dataset_test, batch_size=1, shuffle=False, num_workers=os.cpu_count(),\n    collate_fn=utils.collate_fn)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T12:58:42.812901Z","iopub.execute_input":"2022-06-06T12:58:42.813261Z","iopub.status.idle":"2022-06-06T12:58:42.825589Z","shell.execute_reply.started":"2022-06-06T12:58:42.813229Z","shell.execute_reply":"2022-06-06T12:58:42.824546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# our dataset has 11 classes - background and 10 documents\nnum_classes = 11\n\n# get the model using our helper function\nmodel = build_model(num_classes)\n# move model to the right device\nmodel.to(device)\nprint(model)\n\n# construct an optimizer\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005,\n                            momentum=0.9, weight_decay=0.0005)\n\n# and a learning rate scheduler which decreases the learning rate by\n# 10x every 3 epochs\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                               step_size=3,\n                                               gamma=0.1)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T12:58:49.371166Z","iopub.execute_input":"2022-06-06T12:58:49.371574Z","iopub.status.idle":"2022-06-06T12:58:50.192722Z","shell.execute_reply.started":"2022-06-06T12:58:49.371541Z","shell.execute_reply":"2022-06-06T12:58:50.191838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# number of epochs\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    # train for one epoch, printing every 10 iterations\n    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n    # update the learning rate\n    lr_scheduler.step()\n    # evaluate on the test dataset\n    result = evaluate(model, data_loader_test, device=device)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T12:58:54.135945Z","iopub.execute_input":"2022-06-06T12:58:54.136301Z","iopub.status.idle":"2022-06-06T15:19:07.569997Z","shell.execute_reply.started":"2022-06-06T12:58:54.136269Z","shell.execute_reply":"2022-06-06T15:19:07.568819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model, 'mask-rcnn-document.pt')","metadata":{"execution":{"iopub.status.busy":"2022-06-06T15:25:18.484027Z","iopub.execute_input":"2022-06-06T15:25:18.48444Z","iopub.status.idle":"2022-06-06T15:25:18.814332Z","shell.execute_reply.started":"2022-06-06T15:25:18.484398Z","shell.execute_reply":"2022-06-06T15:25:18.813402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm mask-rcnn-document.pt","metadata":{"execution":{"iopub.status.busy":"2022-06-04T09:17:04.94346Z","iopub.execute_input":"2022-06-04T09:17:04.943862Z","iopub.status.idle":"2022-06-04T09:17:05.709121Z","shell.execute_reply.started":"2022-06-04T09:17:04.943826Z","shell.execute_reply":"2022-06-04T09:17:05.707962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"./mask-rcnn-document.pt\"> Download Model</a>","metadata":{}}]}