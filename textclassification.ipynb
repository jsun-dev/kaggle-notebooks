{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install gdown","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-18T04:35:23.657606Z","iopub.execute_input":"2022-05-18T04:35:23.658185Z","iopub.status.idle":"2022-05-18T04:35:53.316716Z","shell.execute_reply.started":"2022-05-18T04:35:23.658101Z","shell.execute_reply":"2022-05-18T04:35:53.315866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gdown\n\nurl = 'https://drive.google.com/uc?id=1gaEWwTnNEOebN2aTLCDXePVQwkvBwl8W'\n\noutput = 'MIDV-2020-Text.zip'\ngdown.download(url, output)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T04:36:18.361572Z","iopub.execute_input":"2022-05-18T04:36:18.361849Z","iopub.status.idle":"2022-05-18T04:36:25.263315Z","shell.execute_reply.started":"2022-05-18T04:36:18.361817Z","shell.execute_reply":"2022-05-18T04:36:25.262595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip MIDV-2020-Text.zip >/dev/null","metadata":{"execution":{"iopub.status.busy":"2022-05-18T04:36:52.93062Z","iopub.execute_input":"2022-05-18T04:36:52.931118Z","iopub.status.idle":"2022-05-18T04:36:53.652525Z","shell.execute_reply.started":"2022-05-18T04:36:52.93108Z","shell.execute_reply":"2022-05-18T04:36:53.651596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import necessary packages\nfrom tqdm import tqdm\nfrom torch import nn\nfrom torch.optim import Adam\nfrom transformers import BertModel\nfrom transformers import BertTokenizer\nimport os\nimport copy\nimport torch\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-05-18T04:37:22.977958Z","iopub.execute_input":"2022-05-18T04:37:22.978213Z","iopub.status.idle":"2022-05-18T04:37:23.545708Z","shell.execute_reply.started":"2022-05-18T04:37:22.978184Z","shell.execute_reply":"2022-05-18T04:37:23.544928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the name of the dataset\nDATASET_NAME = 'MIDV-2020-Text'\n\n# Determine the device to be used for training and training\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Determine if we will be pinning memory during data loading\nPIN_MEMORY = True if DEVICE == 'cuda' else False\n\n# HYPERPARAMETERS\n# Initialize learning rate, number of epochs, batch size, and momentum\nLEARNING_RATE = 1e-6\nNUM_EPOCHS = 5\nBATCH_SIZE = 2\n\n# Load the labels\nwith open(os.path.join(DATASET_NAME, 'labels.pkl'), 'rb') as f:\n  LABELS = pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T04:37:26.816471Z","iopub.execute_input":"2022-05-18T04:37:26.817085Z","iopub.status.idle":"2022-05-18T04:37:26.872187Z","shell.execute_reply.started":"2022-05-18T04:37:26.817049Z","shell.execute_reply":"2022-05-18T04:37:26.871234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the dataset class\nclass Dataset(torch.utils.data.Dataset):\n  def __init__(self, df, tokenizer):\n    self.labels = [LABELS[label] for label in df['category']]\n    self.texts = [tokenizer(text,\n                            padding='max_length', max_length=512, truncation=True,\n                            return_tensors='pt') for text in df['text']]\n\n  def classes(self):\n    return self.labels\n\n  def __len__(self):\n    return len(self.labels)\n  \n  def get_batch_labels(self, idx):\n    return np.array(self.labels[idx])\n  \n  def get_batch_texts(self, idx):\n    return self.texts[idx]\n  \n  def __getitem__(self, idx):\n    batch_texts = self.get_batch_texts(idx)\n    batch_y = self.get_batch_labels(idx)\n    return batch_texts, batch_y","metadata":{"execution":{"iopub.status.busy":"2022-05-18T04:37:34.553584Z","iopub.execute_input":"2022-05-18T04:37:34.554307Z","iopub.status.idle":"2022-05-18T04:37:34.561031Z","shell.execute_reply.started":"2022-05-18T04:37:34.554248Z","shell.execute_reply":"2022-05-18T04:37:34.560348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the BERT classifier\nclass BertClassifier(nn.Module):\n  def __init__(self):\n    super(BertClassifier, self).__init__()\n    self.bert = BertModel.from_pretrained('bert-base-cased')\n    self.out = nn.Linear(768, len(LABELS))\n  \n  def forward(self, input_id, mask):\n    _, o2 = self.bert(input_ids=input_id, attention_mask=mask, return_dict=False)\n    out = self.out(o2)\n    return out","metadata":{"execution":{"iopub.status.busy":"2022-05-18T04:37:37.694189Z","iopub.execute_input":"2022-05-18T04:37:37.694769Z","iopub.status.idle":"2022-05-18T04:37:37.700037Z","shell.execute_reply.started":"2022-05-18T04:37:37.694731Z","shell.execute_reply":"2022-05-18T04:37:37.699349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the train and test data\ndf_train = pd.read_csv(os.path.join(DATASET_NAME, 'train.csv'))\ndf_test = pd.read_csv(os.path.join(DATASET_NAME, 'test.csv'))\n\n# Create the BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n\n# Create the train and test datasets\ntrain_ds = Dataset(df_train, tokenizer)\ntest_ds = Dataset(df_test, tokenizer)\n\n# Create the train and test data loaders\ntrain_loader = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n                                           num_workers=os.cpu_count(), pin_memory=PIN_MEMORY)\ntest_loader = torch.utils.data.DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False,\n                                          num_workers=os.cpu_count(), pin_memory=PIN_MEMORY)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T04:37:40.661731Z","iopub.execute_input":"2022-05-18T04:37:40.662013Z","iopub.status.idle":"2022-05-18T04:37:51.62988Z","shell.execute_reply.started":"2022-05-18T04:37:40.661984Z","shell.execute_reply":"2022-05-18T04:37:51.629132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the BERT classifer\nmodel = BertClassifier().to(DEVICE)\n\n# Define the loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n\n# Initialize a dictionary to store loss history\nloss_history = {'train_loss': [], 'test_loss': []}\n\n# Initialize a dictionary to store accuracy history\naccuracy_history = {'train_accuracy': [], 'test_accuracy': []}\n\n# Variables to store the best trained model\nbest_model_weights = copy.deepcopy(model.state_dict())\nbest_epoch = 0\nbest_loss = float('inf')\nbest_accuracy = 0.0\n\n# Train the model\nfor e in range(NUM_EPOCHS):\n  total_acc_train = 0\n  total_loss_train = 0\n\n  for train_input, train_label in tqdm(train_loader):\n    train_label = train_label.type(torch.LongTensor)\n    train_label = train_label.to(DEVICE)\n    mask = train_input['attention_mask'].to(DEVICE)\n    input_id = train_input['input_ids'].squeeze(1).to(DEVICE)\n\n    output = model(input_id, mask)\n\n    batch_loss = criterion(output, train_label)\n    total_loss_train += batch_loss.item()\n\n    acc = (output.argmax(dim=1) == train_label).sum().item()\n    total_acc_train += acc\n\n    model.zero_grad()\n    batch_loss.backward()\n    optimizer.step()\n  \n  total_acc_test = 0\n  total_loss_test = 0\n\n  with torch.no_grad():\n    for test_input, test_label in test_loader:\n      test_label = test_label.type(torch.LongTensor)\n      test_label = test_label.to(DEVICE)\n      mask = test_input['attention_mask'].to(DEVICE)\n      input_id = test_input['input_ids'].squeeze(1).to(DEVICE)\n\n      output = model(input_id, mask)\n\n      batch_loss = criterion(output, test_label)\n      total_loss_test += batch_loss.item()\n\n      acc = (output.argmax(dim=1) == test_label).sum().item()\n      total_acc_test += acc\n  \n  train_loss = total_loss_train / len(df_train)\n  train_acc = total_acc_train / len(df_train)\n  test_loss = total_loss_test / len(df_test)\n  test_acc = total_acc_test / len(df_test)\n\n  loss_history['train_loss'].append(train_loss)\n  accuracy_history['train_accuracy'].append(train_acc)\n  loss_history['test_loss'].append(test_loss)\n  accuracy_history['test_accuracy'].append(test_acc)\n\n  print(f'Epoch: {e + 1} | Train Loss: {train_loss: .5f} \\\n            | Train Accuracy: {train_acc: .5f} \\\n            | Test Loss: {test_loss: .5f} \\\n            | Test Accuracy: {test_acc: .5f}')\n\n# Serialize the model to disk\ntorch.save(model, DATASET_NAME + '_model.pt')","metadata":{"execution":{"iopub.status.busy":"2022-05-18T04:38:41.770558Z","iopub.execute_input":"2022-05-18T04:38:41.771002Z","iopub.status.idle":"2022-05-18T05:42:23.760935Z","shell.execute_reply.started":"2022-05-18T04:38:41.770963Z","shell.execute_reply":"2022-05-18T05:42:23.760066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the loss history\nplt.style.use('ggplot')\nplt.figure()\nplt.plot(range(1, NUM_EPOCHS + 1), loss_history['train_loss'], label='train_loss')\nplt.plot(range(1, NUM_EPOCHS + 1), loss_history['test_loss'], label='test_loss')\nplt.title('Training Loss vs Testing Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend(loc='lower left')\nplt.savefig('loss_history.png')","metadata":{"execution":{"iopub.status.busy":"2022-05-18T05:49:46.929732Z","iopub.execute_input":"2022-05-18T05:49:46.930133Z","iopub.status.idle":"2022-05-18T05:49:47.211781Z","shell.execute_reply.started":"2022-05-18T05:49:46.930085Z","shell.execute_reply":"2022-05-18T05:49:47.211096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the accuracy history\nplt.style.use('ggplot')\nplt.figure()\nplt.plot(range(1, NUM_EPOCHS + 1), accuracy_history['train_accuracy'], label='train_accuracy')\nplt.plot(range(1, NUM_EPOCHS + 1), accuracy_history['test_accuracy'], label='test_accuracy')\nplt.title('Training Accuracy vs Testing Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower left')\nplt.savefig('accuracy_history.png')","metadata":{"execution":{"iopub.status.busy":"2022-05-18T05:49:50.502897Z","iopub.execute_input":"2022-05-18T05:49:50.503159Z","iopub.status.idle":"2022-05-18T05:49:50.762811Z","shell.execute_reply.started":"2022-05-18T05:49:50.50313Z","shell.execute_reply":"2022-05-18T05:49:50.762116Z"},"trusted":true},"execution_count":null,"outputs":[]}]}